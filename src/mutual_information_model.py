import numpy as np import mxnet as mx import data_utils from lstm import seq2seq_lstm_inference_encoding_symbol, seq2seq_lstm_inference_decoding_symbolimport loggingfrom seq2seq_model import sentence, Seq2SeqInferenceModelfrom seq2seq_rl_update_model import seq2seqRLUpdateModelclass MutualInformationModel():    def __init__(self, seq_len, batch_size, learning_rate, vocab, rev_vocab, num_hidden, num_embed, num_layers,                forward_arg_params, backward_arg_params, beam_size, data_path, ctx=mx.gpu(0), dropout=0.):        self.MMIGamma = 0.95        self.seq_len = seq_len        self.batch_size = batch_size        self.learning_rate = learning_rate        self.vocab = vocab         self.rev_vocab = rev_vocab        self.num_hidden = num_hidden        self.num_embed = num_embed        self.devs = ctx        self.num_layers = num_layers        self.beam_search_model = Seq2SeqInferenceModel(seq_len, self.batch_size, self.learning_rate,                 self.vocab, self.rev_vocab, self.num_hidden, self.num_embed, self.num_layers,forward_arg_params, beam_size, ctx=self.devs, dropout=0.)        self.backward_prob_model = Seq2SeqInferenceModel(seq_len, self.batch_size, self.learning_rate,                self.vocab, self.rev_vocab, self.num_hidden, self.num_embed, self.num_layers, backward_arg_params, ctx=self.devs, dropout=0.)        self.gradient_update_model = seq2seqRLUpdateModel(seq_len, self.batch_size, self.learning_rate,                self.vocab, self.rev_vocab, self.num_hidden, self.num_embed, self.num_layers, forward_arg_params, ctx=self.devs, dropout=0.)        self.data_path = data_path        self.data = []        self.load_data(self.data_path, self.data)        self.weight_save_path = "../snapshots/mutual_information"    def train(self, epoch):        for num_epoch in range(epoch):            for idx in range(len(self.data)):                pair_idx = self.bucket_idx_all[idx]                p, q = self.data[pair_idx]                self.step(p,q)                #if idx+1 % 3 == 0:                logging.info("Epoch [%d] Batch [%d] Completed" % (num_epoch+1, idx+1))            self.gradient_update_model.save_weights(self.weight_save_path, num_epoch+1)            self.shuffle_data()    def load_data(self, data_path, data):        count = 0        read_file = open(data_path, "r")        last_line = ""        line = read_file.readline()        while line:            if line == last_line:                line = read_file.readline()                continue            elif line == data_utils.file_termimal:                last_line = read_file.readline()                line = read_file.readline()                continue            else:                last_line_ids = data_utils.sentence_to_token_ids(last_line, self.vocab)                line_ids = data_utils.sentence_to_token_ids(line, self.vocab)                if len(last_line_ids) + len(line_ids) < self.seq_len:                    data.append((last_line_ids, line_ids))            last_line = line             line = read_file.readline()        self.shuffle_data()        logging.info("Mutual Information Model Data Load Complete")    def shuffle_data(self):        self.bucket_idx_all = np.random.permutation(len(self.data))    def step(self, p, q):        """        args:        p: a sentence of words(str)/ list of token ids for first sentence as input        q: a sentence of words(str)/ list of token ids for second sentence as input        """        if type(p) == list and type(q) == list:            input_data = list(p) + list(q)            q_token = self.backward_prob_model.tokenize_id_with_seq_len(list(q) + [data_utils.EOS_ID] , reverse=False)        # else:        #     input_data = p + " " + q        #     q_token = self.backward_prob_model.tokenize_id_with_seq_len(q, reverse=False) # q_token type: mx.ndarray        fh = open("prob_pairs.txt","a")        params = self.gradient_update_model.get_weights()        self.beam_search_model.load_params(params)        response_list = self.beam_search_model.forward_beam(input_data)        MMI_value_list = []        for idx in range(len(response_list)):            forward_prob = response.get_cumulative_log_prob()            logging.info(self.beam_search_model.response(response.get_concat_sentence()))            response_tmp = self.beam_search_model.tokenize_id_with_seq_len(response.get_concat_sentence(), reverse=True)            if response_tmp[-1] == data_utils.EOS_ID:                response_tmp.pop()            backward_prob = self.backward_prob_model.get_log_prob(response_tmp, q_token.asnumpy())            MMIValue = self.mutual_information_value(forward_prob, backward_prob)            MMI_value_list.append(MMIValue)        for idx in range(len(response_list)):            MMIValue = MMI_value_list[idx]            MMIValue = MMIValue - sum(MMI_value_list)/len(MMI_value_list)            pred_grad = self.gradient_update_model.forward(input_data, response_list[idx].get_concat_sentence())            gradient = self.mutual_information_gradient(pred_grad, MMIValue, response_list[idx])            self.gradient_update_model.backward(gradient) # calculate and accumulate the gradients            fh.write("%f %f\n"%(forward_prob, backward_prob))        self.gradient_update_model.update_params()        fh.close()    def mutual_information_gradient(self, pred_grad, MMIValue, response):        # sentence = response.get_concat_sentence()        # terminal = False        # logging.info(len(sentence))        # for idx in range(self.seq_len):        #     if terminal:        #         break        #     if sentence[idx] == data_utils.EOS_ID or idx == len(sentence)-1:        #         terminal = True        #     grad = pred_grad[idx, sentence[idx]]        #     pred_grad[idx,:][:] = 0        #     pred_grad[idx,sentence[idx]] = grad * MMIValue        # pred_grad[idx:, :] = 0        # return mx.nd.array(pred_grad, ctx=self.devs)        return mx.nd.array(pred_grad * MMIValue, ctx = self.devs)    def mutual_information_value(self, forward_prob, backward_prob):        return self.MMIGamma * forward_prob + (1 - self.MMIGamma) * backward_prob